---
title: "STA523 HW6"
author: "Shaoji Li, Faustine Li, Eden Huang, Yumemichi Fujita"
date: "11/14/2016"
output: html_document
---

## Setup

```{r, message=FALSE}
library(raster)
library(dplyr)
library(ggplot2)
library(sf)
library(sp)
library(randomForest)
```

## Task 1: Geocoding
Write ups:
In Task 1, we use 2 main packages, dplyr and sf. Especially, by using sf package, we can handle spatial data, read, write, query and manipulate spatial data. After installing the packages, we load data of NYC parking violation data. 
Also, we use `st_read` to read the `pluto` data (spatial data) and read this into R. In this data, there are 42,890 features(rows) and 84 fields (columns). The data we need for this task, is the `Address` and `geometry` from the data, so we put those 2 into a data frame.  From the geometry, we want the center point from a polygon, so we use `st_centroid` to get the latitude and longitude from pluto. Then, we can use `unlist` to flatten the x, y value of the point into a factor, and convert the vector to a matrix, pipe the result. 
From the NYC data, we have house number, and the street name. so we take those 2 columns, and create a new column called `Address`, by pasting those 2 columns together. Also, since we only need data from Manhattan, so we filter the precincts from the NYC data only in Manhattan precincts. After this, we only need to keep `nyc_man` data and `pluto_xy`, so we remove data of nyc and pluto here, to not cause confusion in R, in the same time make the code clearer and efficient. 
After creating the data frame, we move to cleaning the data. 
First, we changed all the address letters to capital, so that all the data are in the same case, to ensure more address will be included. 
Also, we removed NA values because there are no house address, and we removed special characters and double spaces by using a for loop. 
Additionally, we replaced place and street abbreviations with long form so that the address can be in a more concise style also by using a for loop. For example, we use E instead of EAST, PL instead of PLACE, ST instead of STREET and so on.
Another thing we did, is that we remove the ordinal names, such 1st to 1, 2NDto 2, 3RDo 3 by using a for loop again, so that we can get as much as data as possible. All ordinal names, street long form, or letter type will cause less data, so we clean these up to get more data. Then, we fixed the specific newyork street. 
Finally, we used `inner_join` to merge `nyc_man` data and `pluto_xy` data, and saved as `nyc_geo.RDS`


## Task 2: Recreating NYCâ€™s Police Precincts

Load data and plot the precincts on map to get an idea of the boundaries
```{r}
nyc_geo = readRDS(file='nyc_geo.RDS')
latlon = data.frame('lon' = nyc_geo$x, 'lat' = nyc_geo$y)
coord = SpatialPoints(latlon)
plot(coord, col=nyc_geo$precinct, pch=18, cex=0.5, axes=TRUE)
```

Get Manhattan Info

```{r}
nybb = st_read("/data/nyc_parking/nybb/", quiet=TRUE)
manh = nybb %>% filter(BoroName == "Manhattan")
#plot(manh,axes=TRUE)

library(raster)
ext = st_bbox(manh) %>% .[c("xmin","xmax","ymin","ymax")] %>% extent()
r = raster(ext, ncol=100, nrow=300)
r = rasterize(as(manh,"Spatial"),r)
plot(r)
```

Get prediction locations

```{r}
pred_cells = which(!is.na(r[]))
pred_locs = xyFromCell(r, pred_cells)
plot(pred_locs, pch=16, cex=0.1)
```

```{r}
man_precincts = c(1, 5, 6, 7, 9, 10, 13, 14, 17, 18, 19, 20, 22, 23,
                  24, 25, 26, 28, 30, 32, 33, 34)
nyc_geo_reduced = data.frame()
nsamp = 1000
for (i in man_precincts) {
    
    x = nyc_geo[nyc_geo$precinct == i,]$x
    y = nyc_geo[nyc_geo$precinct == i,]$y
    
    # reject samples that are outside 90% quantile
    indx = which(x > quantile(x, 0.05) & x < quantile(x, 0.95))
    indy = which(y > quantile(y, 0.05) & y < quantile(y, 0.95))
    ind = intersect(indx, indy)
    x = x[ind]
    y = y[ind]
    n = length(ind)
    
    if (n > nsamp) {
        # sample nsamp number of observations from each precinct
        k = sample(seq_len(n), nsamp)
        x = x[k]
        y = y[k]
        n = nsamp
    }
    nyc_geo_reduced = rbind(nyc_geo_reduced, cbind(x, y, rep(i, n)))
}
nyc_geo_reduced = setNames(nyc_geo_reduced, c('x', 'y', 'precinct'))
```

```{r}
latlon = data.frame('lon' = nyc_geo_reduced$x, 
                    'lat' = nyc_geo_reduced$y)
coord = SpatialPoints(latlon)
plot(coord, col=nyc_geo_reduced$precinct, pch=18, cex=0.5, axes=TRUE)
```

```{r}
library(xgboost)

precincts = factor(nyc_geo_reduced$precinct) %>% levels()
y = (factor(nyc_geo_reduced$precinct) %>% as.integer()) - 1L
x = nyc_geo_reduced %>% select(x,y) %>% as.matrix()

dtrain = xgb.DMatrix(x, label = y)
dtest  = xgb.DMatrix(pred_locs)

set.seed(0)
xgb_params = list(
    seed = 0,
    colsample_bytree = 0.8,
    subsample = 0.8,
    eta = 0.1,
    objective = 'multi:softmax',
    max_depth = 6,
    min_child_weight = 100
    )

res = xgb.cv(data = dtrain,
             nround = 1000,
             early_stopping_rounds = 20,
             nfold = 5,
             objective = 'multi:softmax',
             num_class = length(precincts))

m = xgboost(data=x, 
            label=y, 
            nthead=4, 
            nround=res$best_iteration, 
            objective="multi:softmax", 
            num_class=length(precincts))

pred_xgb = predict(m, newdata=as.matrix(pred_locs))
pred_xgb = precincts[pred_xg+1]
r.xgb = r
r.xgb[pred_cells] = as.numeric(pred_xgb)
```

Draw the polygons

```{r}
source("polygonizer.R")
p = polygonizer(r.xgb)
p = st_transform(p, 4326)
plot(p)

st_write(p,"precincts.json", "data", driver="GeoJSON", quiet=TRUE)
```

