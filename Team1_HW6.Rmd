---
title: "STA523 HW6"
author: "Shaoji Li, Faustine Li, Eden Huang, Yumemichi Fujita"
date: "11/14/2016"
output: html_document
---

## Setup

## Task 1: Geocoding

In Task 1, we use 2 main packages, `dplyr` and `sf`. `sf` package, in particular, allows us to read, write, query and manipulate spatial data. After installing the packages, we load data of NYC parking violation data. 

For a faster loading speed, we used `st_read` to read the `pluto` data (spatial data) and read this into R. In this data, there are 42,890 features (rows) and 84 fields (columns). The data we need from the dataset for this task is `address` and `geometry`, so we organize those 2 variables into a data frame. The geometric analysis shows that we need the center point from a polygon, so we use `st_centroid` to retrieve the latitude and longitude information from pluto. Then, we use `unlist` to flatten the x, y values of the center points into factors, and convert the vectors to a matrix and pipe the results. 

The NYC data includes house numbers and street names. We subset the dataset into these 2 columns, and create a new column called `Address` by pasting those 2 columns. Since we only need data from Manhattan, we filter the precincts from the NYC data only in Manhattan precincts. After the previous steps, we only need to keep `nyc_man` and `pluto_xy`, and hence remove data of nyc and pluto to avoid confusion in R and improve the efficiency. 

After creating the data frame, we move on to data cleaning on specific New York streets. First, to ensure the inclusion of more address, we changed all the letters of address to capital cases such that all the data are in the same case. Also, we removed NA values where house address does not exist, and removed special characters and double spaces by using a for loop. 

Additionally, we replaced the abbreviations of place and street with concise and standardized forms by using a for loop. For example, we use "E"" instead of "EAST", "PL" instead of "PLACE", "ST" instead of "STREET" and so on.

Note that all ordinal names, street long form, or letter type will lesssen the data entries. Therefore, we also altered some ordinal names, such as from "1st"" to "1"", "2ND" to "2"", "3RD" to "3"" by using another for loop. 

Finally, we used `inner_join` function to merge `nyc_man` and `pluto_xy` data, and saved as `nyc_geo.RDS`. 




## Task 2: Recreating NYCâ€™s Police Precincts

### Part 1
For the second task, we use 3 new packages in R, which are `raster`, `nnet` and `xgboost`. We loaded `raster` prior to `dplyr` to avoid the duplicate mask of `select` function. 

First we load nyc data and rasterize a grid of prediction locations. After reading the `nyc_geo.RDS` file created by task 1, we get the Manhattan information from nybb file under nyc_parking data by using `st_read`. Since we only need information about Manhattan, we use `filter()` to select the rows and get the boundary of Manhattan. 

We move on to find a raster that can cover the area of Manhattan. We need to find a range based on the latitude(x) and longitude(y) of Manhattan boundary using `st_bbox` from `raster` library. But we need to shuffle the order to "xmin","xmax","ymin" and "ymax", since raster requires a specific order. 

Then we use `extent()` to turn the raster to an extent class object. We define a new raster object and break it into 100 columns and 300 rows, 30,000 cells in total. We use `rasterize()` to call polygon, connect to spatial data, and return the raster. Then, we use `!is.na` to get rid of NA value from the data, and use `xyFromCell` function from raster to retrieve the x, y values from raster r and put the values into `pred_locs`.


### Part 2
We move on to reduce the NYC geodata by rejection sampling. We know that in Manhattan, there are precincts from 1 to 34. We use `which` function in a for loop to get rid of the data that outside the 90% quantiles. Using `length()` we can know the number of observations each precinct has. For the precincts with more than 1000 observations, we will randomly choose 1000 samples from that precinct. This method generates 21,209 samples from the original `nyc_geo` data(1,640,133 samples), and hence the data amount is significantly reduced. 

After reducing the model, we added the dummy data for the special case of central park, since parking is not allowed in central park. We get the longitude and latitude of central park for the 4 corners from Google Map. Then, we separate the quadrangle into 2 triangles. The upper triangle has NW, SW and NE corners, and lower triangle has NE, SW and SE corners. We generate equal numbers of dummy points for both triangles, since the areas are similar. Finally, we use `cbind` to combine the x, y and precinct data, and then combine `nyc_with_central` with `nyc_geo_reduced` data using `rbind`. 


### Part 3
To assess the performance of our prediction, we need to fit a model. We use a type of regression model, `xgboost`. The `xgboost` package we use is version 0.6-0, which can be installed using command `install.packages("xgboost", repos="http://dmlc.ml/drat/", type = "source")`.

We first turn the precincts into factors to allow R to match the order in xgboost, and then level them to get the precincts. Then we define y and x variables, where x is our predictor and y is the outcome. In the xgboost function, we turn the precints into factors, record the levels as labels, and then deduce the levels by 1 since xgboost uses labels starting from 0. Also, x will be 21,209 rows by 2 matrices, include x and y from the previous for loops (longitude and latitude). 

To implement our xgboost model, we used a train set and a test set where the train set is the x matrix defined above and the test set is the coordinates of the the locations to be precticted (`pred_locs`). We also use labels from 0 to 21, as there are 22 precincts in Manhatton. We generate the model using `xgboost` from the `xgboost` package using the training data and predict the test data using the `predict` function. To translate the prediction back to the precincts we want, we extract the precinct numbers according to the labels(e.g. precinct 5 corresponds to label 1). Note that since labels in `xgboost` starts from 0, we need to add 1 to all the labels when we subsetting the precinct numbers (e.g. precinct 5 corresponds to label 1, but it's the 2nd item in the vector `precints`). Then, we take all the result values and plug them back into the raster objects to keep oriented in space and  track x and y values. We also change `pred_xgb` into numeric value. 

Finally, we use `polygonizer` given by Collin to find the police districts and plot the graph of polygon. 


```{r, echo=FALSE}
rm(list = ls())
source('precinct_predict.R')
```

```{r, echo=FALSE}
par(mfrow = c(1,2))
latlon = data.frame('lon' = nyc_geo$x, 
                    'lat' = nyc_geo$y)
coord = SpatialPoints(latlon)
plot(coord, col=nyc_geo$precinct, pch=18, cex=0.5, axes=TRUE)

latlon = data.frame('lon' = nyc_geo_reduced$x, 
                    'lat' = nyc_geo_reduced$y)
coord = SpatialPoints(latlon)
plot(coord, col=nyc_geo_reduced$precinct, pch=18, cex=0.5, axes=TRUE)
```

Use polygonizer to create a polygon object:
```{r, echo=FALSE}
plot(p)
```

